---
title: "Dedlearnr Vignette"
author: "Trevor Riley"
date: "`r Sys.Date()`"
output: html_document
---
## About this vignette

# As part of a recent systematic map being conducted on Nature Based Solutions, an initial group of .ris files totaling ~114k were deduplicated using the CiteSource package. Of the ~114k citations ~36k were exported. We knew that a second deduplication process was needed and so the ~36k citations went through manual deduplication in EndNote. The 36k citaitons were deduplicated by Author + Title, Title only, and DOI. Our team wanted to be able to take this cleaner set of citations, post manual deduplication, and analyze them in CiteSource at the end of the project. To do this the duplicate_id and ids columns were exported along with the cite_source column. This provided our team with the ability to choose a primary record to use, while copying over the duplicate_id from the secondary to the primary ids data. The team also copied the cite_source of the secondary record to the primary record. This ensured that we were able to maintain the same data structure that CiteSource provides and allows us to analyze the data after the screening phase.

# The ~36k went down to ~34k citations. While the reduction from ~114k to ~36k is great, we now have an amazing data source in the newly deduplicated records, we know exacly which records were duplicate and how they paired with their counterparts, due to the duplicate_id being merged into the ids. using this link between the pairs I was able to compare the similarities between each of the major fields that could potentially be used to identify duplicates.

## Install and Load Required Libraries
```{r}
#install.packages("remotes")
#library(remotes)
#remotes::install_github("ESHackathon/CiteSource")

# Load required libraries
library(dplyr)
library(tidyr)
library(CiteSource)

```
## Read in citations
```{r}
# Set file path
file_path <- "C:/Users/trevor.riley/Documents/Rwork/Dedlearnr/RIS/"
# Create metadata table
metadata_tbl <- tibble::tribble(
  ~files, ~cite_sources, ~cite_labels,
  "Primary.ris", "primary", NA,
  "Secondary.ris", "secondary", NA
) %>%
  dplyr::mutate(files = paste0(file_path, files))

# Read citations
citations <- read_citations(metadata = metadata_tbl)


```
## Preprocess Citations
```{r}

# Rename columns (for whatever reason Custom 2 was listed as pubmed_id)
citations <- citations %>%
  rename(IDS = pubmed_id, UID = C1)

cleancitations <- select(citations, -c(database, supertaxa, ID, LB, ZZ, language, notes, source_abbreviated, date_generated, ET, date_generated, address))


## Preprocess Citations Function
preprocess_citations <- function(df) {
  # Lowercase all character columns
  lowercase_columns <- function(df) {
    character_columns <- sapply(df, is.character)
    df[, character_columns] <- lapply(df[, character_columns], tolower)
    return(df)
  }
  
  # Remove special characters and extra whitespace
  remove_special_characters <- function(text) {
    return(gsub("[^a-z0-9[:space:]]", "", text))
  }
  trim_whitespace <- function(text) {
    return(gsub("\\s+", " ", trimws(text)))
  }
  clean_text_columns <- function(df) {
    character_columns <- sapply(df, is.character)
    columns_to_clean <- character_columns
    columns_to_clean["IDS"] <- FALSE # exclude the "IDS" column from cleaning
    columns_to_clean[ "start_page"] <- FALSE
    columns_to_clean["author"] <- FALSE
    df[, columns_to_clean] <- lapply(df[, columns_to_clean], remove_special_characters)
    df[, columns_to_clean] <- lapply(df[, columns_to_clean], trim_whitespace)
    return(df)
  }
  
  # Apply preprocessing functions
  df <- lowercase_columns(df)
  df <- clean_text_columns(df)
  
  return(df)
}

cleanercitations <- preprocess_citations(cleancitations)
```
## Filter and Match Citations
```{r}
## Filter Citations
primary_citations <- citations %>% dplyr::filter(cite_source == "primary")
secondary_citations <- citations %>% dplyr::filter(cite_source == "secondary")

## Find Duplicate Pairs
find_matching_citations <- function(primary_citations, secondary_citations) {
  duplicate_pairs <- list()
  
  for (i in 1:nrow(secondary_citations)) {
    secondary_uid <- secondary_citations$UID[i]
    
    for (j in 1:nrow(primary_citations)) {
      primary_ids <- strsplit(primary_citations$IDS[j], split = ",")[[1]]
      primary_ids <- trimws(primary_ids) # Remove whitespace from primary_ids
      
      if (secondary_uid %in% primary_ids) {
        duplicate_pairs[[length(duplicate_pairs) + 1]] <- list(primary_citation = primary_citations[j, ], secondary_citation = secondary_citations[i, ])
      }
    }
  }
  
  return(duplicate_pairs)
}
# Find duplicate pairs
duplicate_pairs <- find_matching_citations(primary_citations, secondary_citations)
```

## Calculate Similarities
```{r}
## Calculate Field Similarities
jaccard_similarity <- function(set1, set2) {
  intersection_size <- length(intersect(set1, set2))
  union_size <- length(union(set1, set2))
  
  if (union_size == 0) {
    return(0)
  }
  
  return(intersection_size / union_size)
}
compute_field_similarities <- function(duplicate_pair, selected_fields) {
  primary_citation <- duplicate_pair$primary_citation
  secondary_citation <- duplicate_pair$secondary_citation
  
  field_similarities <- list()
  
  for (field_name in selected_fields) {
    primary_field <- strsplit(primary_citation[[field_name]], split = "\\s+")[[1]]
    secondary_field <- strsplit(secondary_citation[[field_name]], split = "\\s+")[[1]]
    
    field_similarity <- jaccard_similarity(primary_field, secondary_field)
    field_similarities[[field_name]] <- field_similarity
  }
  
  return(field_similarities)
}

# Select the specific fields you want to compare
selected_fields <- c("title", "source", "volume", "issue", "abstract", "doi", "author")

# Calculate the field similarities for each duplicate pair using the selected fields
field_similarities_list <- lapply(duplicate_pairs, compute_field_similarities, selected_fields = selected_fields)

# Convert the list of field similarities into a data frame
field_similarities_df <- do.call(rbind, lapply(field_similarities_list, function(x) data.frame(t(unlist(x)))))

## Summary Statistics
library(tibble)
summary_statistics <- function(df) {
  min_values <- apply(df, 2, min)
  max_values <- apply(df, 2, max)
  mean_values <- apply(df, 2, mean)
  median_values <- apply(df, 2, median)
  
  summary_df <- data.frame(
    min = min_values,
    max = max_values,
    mean = mean_values,
    median = median_values,
    stringsAsFactors = FALSE
  )
  
  return(summary_df)
}
# Calculate the summary statistics for each field
field_summary <- summary_statistics(field_similarities_df)
# Calculate the average similarity score for each duplicate pair
average_similarity <- rowMeans(field_similarities_df)
# Combine the field summary and average similarity into a single table
summary_table <- field_summary %>%
  bind_rows(tibble(pair = paste0("Pair ", 1:length(average_similarity)), average_similarity = average_similarity, check.names = FALSE)) %>%
  t() %>% as.data.frame() %>%
  rownames_to_column("Field")
```

## Create Similarity Tables
```{r}
# Table 1: Average similarity across all pairs for each field
field_avg_similarity <- data.frame(
  Field = colnames(field_similarities_df),
  Avg_Similarity = sapply(field_similarities_df, mean),
  stringsAsFactors = FALSE
)
# Table 2: Calculated scores for each pair with overall similarity score
pair_scores <- field_similarities_df %>%
  mutate(
    Pair = paste0("Pair ", 1:nrow(field_similarities_df)),
    Overall_Similarity = rowMeans(field_similarities_df)
  ) %>%
  select(Pair, everything())
```
## Visualize Similarity Scores

```{r}
library(plotly)

plot1<-ggplot(pair_scores_long, aes(x = Similarity, fill = Field)) +
  geom_density(alpha = 0.4) +
  labs(x = "Similarity Score", y = "Density") +
  theme_minimal()

plot2<-ggplot(pair_scores_long, aes(x = Similarity, fill = Field)) +
  geom_density(alpha = 0.6, position = "stack") +
  labs(x = "Similarity Score", y = "Density") +
  theme_minimal()

plot3<- ggplot(pair_scores_long, aes(x = Similarity, fill = Field)) +
  geom_density(alpha = 0.6) +
  facet_wrap(~ Field, nrow = 2) +
  labs(x = "Similarity Score", y = "Density") +
  theme_minimal()

plot4 <- ggplot(pair_scores_long, aes(x = Field, y = Similarity, color = Field)) +
  geom_jitter(alpha = 0.6) +
  labs(x = "Field", y = "Similarity Score") +
  theme_minimal()


funplot1 <- ggplotly(plot1)
funplot2 <- ggplotly(plot2)
funplot3 <- ggplotly(plot3)
funplot4 <- ggplotly(plot4)


plot1
plot2
plot3
plot4

funplot1
funplot2
funplot3
funplot4
```
```{r}
#now that we can visualize this data I should upload the rest of the data that we manually deduplicated. this data was only based on title/author (I believe) the other data sets would be DOI matches and the third was only title.

#looking at this inital data set we can say with some confidence that matching abstracts are a good indicator. However this is probably the easiest way to get false positives due to the reuse of abstracts between article/chapters and prepring/articles. Plot2 the stacked density plot shows a large number of high-similarity pairs across various elements. The high number of elements at the 0 end is most likely blank data in that element.


```
